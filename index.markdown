---
layout: common
permalink: /
categories: projects
---

<link media="all" href="./css/glab.css" type="text/css" rel="StyleSheet">
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LEGATO: Cross-Embodiment Imitation Using a Grasping Tool</title>

<!-- <meta property="og:image" content="src/figure/approach.png"> -->
<meta property="og:title" content="LEGATO">

<script src="./src/popup.js" type="text/javascript"></script>
<script src="/src/js/viewstl/stl_viewer.min.js"></script>
<script src="/src/js/viewstl/init.js"></script>
<script src="https://kit.fontawesome.com/ef67f68cfb.js" crossorigin="anonymous"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LLEPNK1F3W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LLEPNK1F3W');
</script>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('video.lazy-video');
    
    const observer = new IntersectionObserver(entries => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.play();
        } else {
          entry.target.pause();
        }
      });
    }, {
      threshold: 0.5 // Adjust this as needed (0.5 means 50% of the video must be visible)
    });
    
    videos.forEach(video => {
      observer.observe(video);
    });
  });
</script>

<!-- STLviewer tag -->
<!-- <script>
function initStlViewer() {
    var $modelElements = $("div.3d-model");
    $modelElements.each(function (i, elem) {
        var filePath = $(elem).data('src');
        console.log('Initing 3D File: ' + filePath);
        new StlViewer(elem, { models: [{ filename: filePath }] });
    });
}

$(document).ready(initStlViewer);
</script> -->

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<style type="text/css" media="all">
body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 100%;
  }
.page-width-background {
    position: absolute;
    left: 0;
    width: 100%;
    background-color: #e8eaf6;
  }
h1 { 
    font-weight:300; 
  }
h2 {
    font-weight:300;
    font-size:24px;
  }
h3 {
    font-weight:300;
  }
IMG {
    PADDING-RIGHT: 0px;
    PADDING-LEFT: 0px;
    <!-- FLOAT: justify; -->
    PADDING-BOTTOM: 0px;
    PADDING-TOP: 0px;
    display:block;
    margin:auto;  
  }
#primarycontent {
    MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
    1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
    1000px 
  }
BODY {
    TEXT-ALIGN: center
  }
hr{
    border: 0;
    height: 1px;
    max-width: 1100px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
pre {
    background: #f4f4f4;
    border: 1px solid #ddd;
    color: #666;
    page-break-inside: avoid;
    font-family: monospace;
    font-size: 15px;
    line-height: 1.6;
    margin-bottom: 1.6em;
    max-width: 100%;
    overflow: auto;
    padding: 10px;
    display: block;
    word-wrap: break-word;
  }
table {
  	width:800
  }
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script
src="./src/b5m.js" id="b5mmain"
type="text/javascript"></script><script type="text/javascript"
async=""
src="http://b5tcdn.bang5mai.com/js/flag.js?v=156945351"></script>


</head>

<body data-gr-c-s-loaded="true">


<style>
a {
  color: #800080;
  text-decoration: none;
  font-weight: 500;
}
</style>


<style>
highlight {
  color: #ff0000;
  text-decoration: none;
}
</style>
<div id="primarycontent">
<div style="height: 4px;"></div>
<center>
  <h1>
    <strong>LEGATO: Cross-Embodiment Imitation Using a Grasping Tool</strong>
  </h1>
</center>
<center>
  <h3>
    <a href="https://mingyoseo.com/">Mingyo Seo<sup>1,2</sup></a>&nbsp;&nbsp;&nbsp;
    <a href="https://www.linkedin.com/in/robodreamer/">H. Andy Park<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
    <a href="https://yuanshenli.com/">Shenli Yuan<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
    <a href="https://yukezhu.me/">Yuke Zhu<sup>1&dagger;</sup></a>&nbsp;&nbsp;&nbsp;
    <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/sentis/">Luis Sentis<sup>1,2&dagger;</sup></a>&nbsp;&nbsp;&nbsp; 
  </h3>
  <h3>
    <a href="https://www.utexas.edu/"><sup>1</sup>The University of Texas at Austin</a>&nbsp;&nbsp;&nbsp;
    <a href="https://theaiinstitute.com/"><sup>2</sup>The AI Institute</a>&nbsp;&nbsp;&nbsp;
    <sup>&dagger;</sup> Equal advising
  </h3>
  <h3>IEEE Robotics and Automation Letters (RA-L), 2025</h3>
  <h3>
    <a href="http://arxiv.org/abs/2411.03682">
      <i class="ai ai-arxiv"></i> Paper</a> | 
    <a href="https://github.com/UT-HCRL/LEGATO">
      <i class="fa-brands fa-github"></i> Code</a> | 
    <a href="https://ut-hcrl.github.io/LEGATO-Gripper">
      <i class="fa-solid fa-gear"></i> Hardware</a> |
    <a href="./src/file/appendix.pdf" download>
      <i class="fa-solid fa-file-pdf"></i> Appendix</a>
  </h3>
</center>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <video muted autoplay loop width="798">
          <source src="./src/video/header.mp4"  type="video/mp4">
        </video>
      </td>
    </tr> 
  </tbody> 
</table>

<p>
  <div width="500">
    <p>
      <table align=center width=800px>
        <tr>
          <td>
            <p align="justify" width="20%">
              Cross-embodiment imitation learning enables policies trained on specific embodiments to transfer across different robots, unlocking the potential for large-scale imitation learning that is both cost-effective and highly reusable. 
              This paper presents <b>LEGATO</b>, a cross-embodiment imitation learning framework for visuomotor skill transfer across varied kinematic morphologies. 
              We introduce a handheld gripper that unifies action and observation spaces, allowing tasks to be defined consistently across robots. 
              We train visuomotor policies on task demonstrations using this gripper through imitation learning, applying transformation to a motion-invariant space for computing the training loss. 
              Gripper motions generated by the policies are retargeted into high-degree-of-freedom whole-body motions using inverse kinematics for deployment across diverse embodiments. 
              Our evaluations in simulation and real-robot experiments highlight the framework's effectiveness in learning and transferring visuomotor skills across various robots.
      	    </p>
          </td>
        </tr>
      </table>
    </p>
  </div>
</p>

<hr>
<h1 align="center">Cross-Embodiment Learning Pipeline</h1>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <a href="./src/figure/overview.png"> 
          <img src="./src/figure/overview.png" style="width:598;">
        </a>
      </td>
    </tr> 
  </tbody> 
</table>

<table align=center width=800px>
  <tr>
    <td>
      <p align="justify" width="20%">
        We introcuce a cross-embodiment imitation learning framework that enables human demonstrations via direct interaction or robot teleoperation.
        Our framework uses the LEGATO Gripper, a versatile handheld grasping tool that ensures consistent physical interactions across different embodiments.
        During data collection, the LEGATO Gripper records its trajectories, grasping actions, and visual observations captured by its egocentric stereo camera.
        Visuomotor policies trained on demonstrations by humans or teleoperated robots using the tool can be deployed across various robots equipped with the same gripper.
        Motion retargeting enables the execution of trajectories on different robots without requiring robot-specific training data.
      </p>
    </td>
  </tr>
</table>

<hr>
<h1 align="center">Handheld Gripper Design</h1>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr> 
      <td align="center" valign="middle">
        <video muted autoplay loop width="394">
          <source src="./src/video/mechanism.mp4"  type="video/mp4">
        </video>
      </td>
    </tr> 
  </tbody> 
</table>
<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <a href="./src/figure/hardware.png"> 
          <img src="./src/figure/hardware.png" style="width:394;">
        </a>
      </td>
      <td align="center" valign="middle">
        <video muted autoplay loop width="394">
          <source src="./src/video/assembly.mp4"  type="video/mp4">
        </video>
      </td>
    </tr> 
  </tbody> 
</table>

<table align=center width=800px>
  <tr>
    <td> 
      <p align="justify" width="20%">
        The LEGATO Gripper is designed for both human demonstration collection and robot deployment.
        It features a shared actuated gripper with adaptable handles, ensuring reliable human handling and consistent grasping across robots while minimizing components.
      </p>
    </td>
  </tr>
</table>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <video muted controls loop width="394">
          <source src="./src/video/usage_human.mp4"  type="video/mp4">
        </video>
      </td>
      <td align="center" valign="middle">
        <video muted controls loop width="394">
          <source src="./src/video/usage_robot.mp4"  type="video/mp4">
        </video>
      </td>
    </tr>
  </tbody>
</table>

<table align=center width=800px>
  <tr>
    <td> 
      <p align="justify" width="20%">
        A human demonstrator can directly perform tasks by carrying the LEGATO Gripper in hand. 
        The LEGATO Gripper is easily installable on various robots, securely held by their original grippers, and is ready for immediate use.
      </p>
    </td>
  </tr>
</table>

<hr>
<h1 align="center">Whole-body Motion Retargeting</h1>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <video class="lazy-video" muted loop width="798">
          <source src="./src/video/whole-body_ik.mp4"  type="video/mp4">
        </video>
      </td>
    </tr> 
  </tbody> 
</table>

<table align=center width=800px>
  <tr>
    <td> 
      <p align="justify" width="20%">
        Motion retargeting through IK optimization adeptly navigates the kinematic differences and constraints across robot embodiments, exploiting kinematic redundancy without requiring additional robot-specific demonstrations for deployment.
      </p>
    </td>
  </tr>
</table>

<hr>
<h1 align="center">Real-Robot Deployment</h1>
<table align=center width=800px>
  <tr>
    <td> 
      <p align="justify" width="20%">
        We trained visuomotor policies on direct human demonstrations and successfully deployed them on the <i>Panda</i> robot system. Our method succeeded in 16 trials of the <i>Closing the lid</i> task, 13 trials of the <i>Cup shelving</i> task, and 14 trials of the <i>Ladle reorganization</i> task, respectively.
      </p>
    </td>
  </tr>
</table>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <video class="lazy-video" muted loop  width="598">
          <source src="./src/video/real_lid.mp4"  type="video/mp4">
        </video>
      </td>
    </tr>
    <tr>
      <td align="center" valign="middle">
        <video class="lazy-video" muted loop  width="598">
          <source src="./src/video/real_cup.mp4"  type="video/mp4">
        </video>
      </td>
    </tr>
    <tr>
      <td align="center" valign="middle">
        <video class="lazy-video" muted loop  width="598">
          <source src="./src/video/real_ladle.mp4"  type="video/mp4">
        </video>
      </td>
    </tr>
  </tbody>
</table>

<hr>
<h1 align="center">Simulation Evaluation</h1>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <a href="./src/figure/benchmark.png">
          <img src="./src/figure/benchmark.png" style="width:798;">
        </a>
      </td>
    </tr>
  </tbody>
</table>

<table align=center width=800px>
  <tr>
    <td>
      <p align="justify" width="20%">
    	On average, LEGATO  outperforms the other methods in cross-embodiment deployment by 28.9%, 10.5%, and 21.1%, compared to <a href="https://arxiv.org/abs/2108.03298">BC-RNN</a>, <a href="https://arxiv.org/abs/2303.04137v5">Diffusion Policy</a>, and the self-variant of LEGATO trained only on SE3 (LEGATO (SE3)), respectively.
    	Notably, unlike the baselines that only achieved high success rates on specific robot bodies, typically the <i>Abstract</i> embodiment used for training, LEGATO demonstrates consistent success across different embodiments.
      </p>
    </td>
  </tr>
</table>

<table border="0" cellspacing="10" cellpadding="0" align="center">
  <tbody>
    <tr>
      <td align="center" valign="middle">
        <video class="lazy-video" muted loop width="598">
          <source src="./src/video/sim_lid.mp4"  type="video/mp4">
        </video>
      </td>
    </tr>
    <tr>
      <td align="center" valign="middle">
        <video class="lazy-video" muted loop width="598">
          <source src="./src/video/sim_cup.mp4"  type="video/mp4">
        </video>
      </td>
    </tr>
    <tr>
      <td align="center" valign="middle">
        <video class="lazy-video" muted loop width="598">
          <source src="./src/video/sim_ladle.mp4"  type="video/mp4">
        </video>
      </td>
    </tr>
  </tbody>
</table>

<hr>
<center><h1>Citation</h1></center>

<table align=center width=800px>
  <tr>
    <td>
    <!-- <left> -->
    <pre><code style="display:block; overflow-x: auto">
      @article{seo2024legato,
        title={LEGATO: Cross-Embodiment Imitation Using a Grasping Tool},
        author={Seo, Mingyo and Park, H. Andy and Yuan, Shenli and Zhu, Yuke
          and Sentis, Luis},
        journal={IEEE Robotics and Automation Letters (RA-L)},
        year={2025}
      }
    </code></pre>
    <!-- </left> -->
    </td>
  </tr>
</table>

<div class="page-width-background">
<div style="height: 4px;"></div>
<center><h2 align="center">Acknowledgement</h2></center>
<table align=center width=800px>
  <tr>
    <td> 
      <p align="justify" width="20%">
        This work was conducted during Mingyo Seo's internship at the AI Institute. We thank Rutav Shah and Minkyung Kim for providing feedback on this manuscript. We thank Osman Dogan Yirmibesoglu for designing the fin ray style compliant fingers and helping with hardware prototyping. We thank Mitchell Pryor and Fabian Parra for their support with the real Spot demonstration. We acknowledge the support of the AI Institute and the Office of Naval Research (N00014-22-1-2204).
      </p>
    </td>
  </tr>
</table>
<div style="height: 16px;"></div>
</div>
